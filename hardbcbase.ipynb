import os
import sys
import scipy.special
import math
import numpy as np
import jax
import jax.numpy as jnp
import jax.nn as jnn
from jax import random, jit, grad, vmap, jacfwd, jacrev
import optax
from jaxopt import LBFGS
import matplotlib.pyplot as plt
from matplotlib import cm
import matplotlib as mpl
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Define the domain boundaries - changing only these to match our 1D advection-diffusion problem
xmin, xmax = 0.0, 1.0 # Domain length (L₁ = 1.0)
tmin, tmax = 0.0, 1.0 # Time domain

# Define the number of points - keeping the same structure but increasing density in regions
nxb = 101 

# CHANGE 1: Reduce number of collocation points from 8000 to 2000
N_r = 2000  # Reduced from 8000 to 2000 as suggested

nx = 1001
nt = 1001

# Initialize random key
key = random.PRNGKey(42)
key1, key2, key3, key4, key5, key6, key7, key8 = random.split(key, 8)

# Generate collocation points with higher density in early time and space regions
# CHANGE 2: Modified distribution to focus on [0, 0.1] instead of [0, 0.2] for better error resolution

# First, generate a portion of points with uniform distribution (50%)
n_uniform = N_r // 2
x_uniform = random.uniform(key1, shape=(n_uniform,), minval=xmin, maxval=xmax)
t_uniform = random.uniform(key2, shape=(n_uniform,), minval=tmin, maxval=tmax)

# CHANGE 3: Generate points with higher density in x = [0, 0.1] and t = [0, 0.1] (50%)
# For x, use beta distribution with stronger bias towards the left side (x = 0 to 0.1)
n_biased = N_r - n_uniform
# Updated beta parameters to focus more on [0, 0.1] region
x_dense_region = random.beta(key3, a=1.0, b=9.0, shape=(n_biased,)) * xmax 
# Updated beta parameters for time to focus on [0, 0.1]
t_dense_region = random.beta(key4, a=1.0, b=9.0, shape=(n_biased,)) * tmax

# Combine the uniform and biased points
x = jnp.concatenate([x_uniform, x_dense_region])
t = jnp.concatenate([t_uniform, t_dense_region])

# Stack into collocation points
colloc = jnp.stack([x, t], axis=1)

# Add extra collocation points specifically along t=0 for better initial condition handling
N_ic_extra = 200  # Reduced from 500 since we're reducing overall points
x_ic_extra = random.uniform(key7, shape=(N_ic_extra,), minval=xmin, maxval=xmax)
t_ic_extra = jnp.zeros(N_ic_extra)
ic_extra_colloc = jnp.stack([x_ic_extra, t_ic_extra], axis=1)

# Combine all collocation points
colloc = jnp.concatenate([colloc, ic_extra_colloc])

# Create a histogram to visualize the distribution of collocation points
plt.figure(figsize=(10, 8))
plt.subplot(2, 1, 1)
plt.hist(x, bins=50, alpha=0.7)
plt.title('Distribution of x-coordinates in collocation points')
plt.xlabel('x')
plt.ylabel('Count')

plt.subplot(2, 1, 2)
plt.hist(t, bins=50, alpha=0.7)
plt.title('Distribution of t-coordinates in collocation points')
plt.xlabel('t')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# Create a scatter plot to visualize the distribution of collocation points
plt.figure(figsize=(8, 8))
plt.scatter(colloc[:, 0], colloc[:, 1], s=1, alpha=0.5)
plt.xlabel('x')
plt.ylabel('t')
plt.title('Distribution of Collocation Points')
plt.xlim(xmin, xmax)
plt.ylim(tmin, tmax)
plt.grid(True)
plt.show()

# Define boundary points
N_b = 200
xb = random.uniform(key5, shape=(N_b,), dtype='float32', minval=xmin, maxval=xmax)
t0 = jnp.stack([xb, jnp.ones(N_b)*tmin], 1)

# Define boundary points
x0_points = jnp.stack([jnp.zeros(N_b), random.uniform(key6, shape=(N_b,), minval=tmin, maxval=tmax)], 1)  # Left boundary x=0
xL_points = jnp.stack([jnp.ones(N_b)*xmax, random.uniform(key8, shape=(N_b,), minval=tmin, maxval=tmax)], 1)  # Right boundary x=L₁

# Neural Network Architecture
def neural_net(params, x, y):
    """
    Forward pass of a neural network to predict P(x, y).

    Inputs:
    params: Neural network parameters (list of weights and biases).
    x, y: Input coordinates (N x 1).

    Output:
    Predicted P(x, y) values (N x 1).
    """
    X = jnp.concatenate([x, y], axis=1)

    *hidden, last = params

    for layer in hidden:
        X = jax.nn.tanh(X@layer['W']+layer['B'])

    return X@last['W'] + last['B']

# CHANGE: Hard constraint neural network wrapper following the equation (base_nn*x)+(1.0-x)
def neural_net_with_hard_constraint(params, x, t):
    """
    Forward pass of a neural network with hard-coded boundary conditions:
    u(x,t) = base_nn(x,t)*x + (1.0-x)
    
    This ensures:
    - At x=0: u(0,t) = 1.0 (left boundary condition)
    - The neural network has built-in constraint for the boundary
    
    Inputs:
    params: Neural network parameters
    x, t: Input coordinates (N x 1)
    
    Output:
    u(x,t) with hard-coded boundary conditions
    """
    # Get the base neural network output
    base_nn_output = neural_net(params, x, t)
    
    # Apply the hard constraint formula: (base_nn*x)+(1.0-x)
    # This ensures u(0,t) = 1.0 for all t (left boundary)
    u_constrained = base_nn_output * x + (1.0 - x)
    
    return u_constrained

# Define the PDE residual for the 1D advection-diffusion equation
def pde_residual_2d_darcy(x, t, u):
    """
    Compute the PDE residual for the 1D advection-diffusion equation:
    ∂C/∂t = 0.1 * (-∂C/∂x + ∂²C/∂x²)

    Inputs:
    x, t: Coordinates of collocation points.
    u: Lambda function representing the neural network solution u(x, t).

    Output:
    Residual of the PDE at the collocation points and initial condition residual.
    """
    # Define the derivatives using JAX automatic differentiation
    u_x = lambda x, t: jax.grad(lambda x, t: jnp.sum(u(x, t)), 0)(x, t)
    u_xx = lambda x, t: jax.grad(lambda x, t: jnp.sum(u_x(x, t)), 0)(x, t)
    u_t = lambda x, t: jax.grad(lambda x, t: jnp.sum(u(x, t)), 1)(x, t)
    
    # Initial condition: C(x,t=0) = 0
    ic1 = u(t0[:,[0]], t0[:,[1]]).reshape(-1,1) - 0.0
    
    # Return the PDE residual (advection-diffusion equation) and initial condition
    # ∂C/∂t = 0.1 * (-∂C/∂x + ∂²C/∂x²)
    return u_t(x, t) - 0.1 * (-u_x(x, t) + u_xx(x, t)), ic1


# Function to initialize parameters
def init_params(layers):
    """
    Initialize parameters (weights and biases) for a neural network with specified layers.

    Args:
    layers: List of integers representing the number of nodes in each layer.
    For example, [2, 20, 20, 1] creates a network with input layer (2 nodes),
    two hidden layers (20 nodes each), and an output layer (1 node).

    Returns:
    params: List of dictionaries containing 'W' (weights) and 'B' (biases) for each layer.
    """
    keys = jax.random.split(jax.random.PRNGKey(0), len(layers) -1) # Generate random keys for each layer
    params = list() # Initialize a list to store layer parameters
    for key, n_in, n_out in zip(keys, layers[:-1], layers[1:]):
    # Loop through layer dimensions
        lb, ub = -(1 / jnp.sqrt(n_in)), (1 / jnp.sqrt(n_in))
        # Xavier initialization bounds
        W = lb + (ub - lb) * jax.random.uniform(key, shape=(n_in, n_out)) 
        # Initialize weights
        B = jax.random.uniform(key, shape=(n_out,)) 
        # Initialize biases
        params.append({'W': W, 'B': B}) # Append layer parameters (weights and biases) to the list
    return params 


@jax.jit
def loss_fun(params, colloc):
    """
    CHANGE: Modified to use hard constraints instead of soft constraints.
    Calculate the total loss for the PINN with hard constraints for boundary conditions.

    Inputs:
    params: Neural network parameters (list of weights and biases).
    colloc: Collocation points for the PDE residual (N x 2).

    Output:
    Scalar total loss value.
    """
    x_c, t_c = colloc[:, [0]], colloc[:, [1]]
    
    # Time-dependent weighting factor with higher emphasis on early times
    lambda_t = (200.0*(1.0-(t_c-tmin)/(0.1))**2 + 1.0) * (t_c <= 0.1) + \
              (20.0*(1.0-(t_c-tmin)/(tmax-tmin))**2 + 1.0) * (t_c > 0.1)
    lambda_t = lambda_t.reshape(-1,1)
    
    # Define the neural network with hard-coded boundary conditions
    u_nn = lambda x, t: neural_net_with_hard_constraint(params, x, t)
    
    pde_residual, ic1 = pde_residual_2d_darcy(x_c, t_c, u_nn)
    
    # Calculate losses - no boundary condition losses as they're handled by the hard constraint
    pde_loss = jnp.mean(lambda_t * pde_residual**2)
    ic_loss = 100.0 * jnp.mean(ic1**2)  # Higher weight for initial condition loss
    
    # Total loss - no boundary condition losses since they're enforced by construction
    loss = pde_loss + ic_loss

    return loss


# Define hyperparameters
hidden_nodes = 40
hidden_layers = 5
lr = 5e-4
epochs = 20000
epochs_lbfgs = 250
batch_size = 512

# Initialize neural network parameters
params = init_params([2] + [hidden_nodes] * hidden_layers + [1])

# Define the optimizer
optimizer = optax.adam(lr)
# Initialize the optimizer state
opt_state = optimizer.init(params)

# Parameters update function 
@jax.jit
def update(opt_state, params, colloc):
    """
    Perform a single optimization step:
    - Calculate gradients of the loss with respect to network parameters.
    - Update the parameters using the optimizer.

    Inputs:
    opt_state: Current optimizer state.
    params: Current neural network parameters (weights and biases).
    colloc: Collocation points for the PDE residual loss.

    Outputs:
    opt_state: Updated optimizer state.
    params: Updated network parameters.
    """
    # Get the gradient w.r.t to MLP params
    value, grads = jax.value_and_grad(loss_fun)(params, colloc)
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    return opt_state, params, value


# LBFGS update function
@jax.jit
def update_lbfgs(params, colloc):
    """ Compute the gradient for a batch and update the parameters using bfgs """
    solver = LBFGS(fun=loss_fun, value_and_grad=False, has_aux=False, maxiter=500, tol=1e-6, stepsize=0.0)
    res = solver.run(params, colloc)
    return res.params


def analytical_sol(x, t):
    if t == 0:
        return 0.0
    c1 = (scipy.special.erfc((x - (0.1 * t)) / (2 * (math.sqrt(0.1 * t)))))
    c2 = (math.e ** (x)) * scipy.special.erfc((x + 0.1 * t) / (2 * math.sqrt(0.1 * t)))
    return 0.5*(c1 + c2)


# PINN training loop
print('PINN training started...')

# Initialize tracking variables
best_params = params
best_loss = float('inf')
best_epoch = 0

# Initialize lists to track all losses and epochs
all_losses = []
all_epochs = []

# Training loop
for epoch in range(epochs+1):
    # Update the optimizer state and parameters
    colloc_batched = colloc[np.random.choice(colloc.shape[0], size=batch_size)]
    opt_state, params, current_loss = update(opt_state, params, colloc_batched)

    # Store loss and epoch
    all_losses.append(current_loss)
    all_epochs.append(epoch)

    # Update the best parameters if the current loss is the lowest
    if current_loss < best_loss:
        best_loss = current_loss
        best_params = params
        best_epoch = epoch
        
    # Print loss and epoch info
    if epoch % 1000 == 0:
        print(f'Epoch={epoch}\tloss={current_loss:.3e}')

print(f'Best Epoch ={best_epoch}\tBest Loss = {best_loss:.3e}')

# LBFGS fine-tuning
print('Starting LBFGS fine-tuning...')
for epoch in range(epochs_lbfgs+1):
    # Update the optimizer state and parameters
    colloc_batched = colloc[np.random.choice(colloc.shape[0], size=batch_size)]
    params = update_lbfgs(params, colloc_batched)

    # Calculate the current loss
    current_loss = loss_fun(params, colloc_batched)
    
    # Store loss and epoch
    all_losses.append(current_loss)
    all_epochs.append(epoch+epochs)
    
    # Update the best parameters if the current loss is the lowest
    if current_loss < best_loss:
        best_loss = current_loss
        best_params = params
        best_epoch = epoch+epochs

    # Print loss and epoch info
    if epoch % 10 == 0:
        print(f'Epoch={epoch+epochs}\tloss={current_loss:.3e}')
        
print('PINN training done!')
print(f'Best Epoch ={best_epoch}\tBest Loss = {best_loss:.3e}')

# Define the trained neural network with hard constraints
u_trained = lambda x, t: neural_net_with_hard_constraint(best_params, x, t)

# Create a regular grid for visualization
x_vis = jnp.linspace(xmin, xmax, 100)
t_vis = jnp.linspace(tmin, tmax, 100)
X, T = jnp.meshgrid(x_vis, t_vis)
# Flatten the grid for prediction
X_flat = X.flatten().reshape(-1, 1)
T_flat = T.flatten().reshape(-1, 1)

# Make predictions using the trained neural network
C_pred = []
C_error = []
C_act = []
# Loop through the flattened arrays
for i in range(len(X_flat)):
    # Get the prediction for the current input
    C_pred.append(u_trained(X_flat[i:i+1], T_flat[i:i+1])[0][0])
    C_act.append(analytical_sol(X_flat[i].item(), T_flat[i].item()))
    C_error.append(abs(analytical_sol(X_flat[i].item(), T_flat[i].item())-u_trained(X_flat[i:i+1], T_flat[i:i+1])[0][0]))
C_pred = jnp.array(C_pred).flatten()
# Reshape the predictions to match the grid
C_grid = C_pred.reshape(X.shape)
C_act = jnp.array(C_act).flatten()
C_error = jnp.array(C_error).flatten()
# Reshape the predictions to match the grid
C_act_grid = C_act.reshape(X.shape)
C_error_grid = C_error.reshape(X.shape)

# Plot the loss history
plt.figure(figsize=(10,6))
plt.semilogy(all_epochs, all_losses)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss History')
plt.grid(True)
plt.show()

# Plot concentration profiles at different times
plt.figure(figsize=(10, 6))
# Define the time points for which to plot the concentration profiles
# Added more early time points (0.01, 0.05, 0.1) for better visualization
time_points = [0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0]
time_indices = [np.argmin(np.abs(t_vis - t_val)) for t_val in time_points]

# Create the plot
plt.figure(figsize=(10, 6))

# Plot concentration profiles at specified time points
for t_idx in time_indices:
    plt.plot(x_vis, C_grid[t_idx, :], label=f'Predicted t = {time_points[time_indices.index(t_idx)]:.2f}')
    plt.plot(x_vis, C_act_grid[t_idx, :], linestyle='--', label=f'Actual t = {time_points[time_indices.index(t_idx)]:.2f}', alpha=0.7)

# Set plot limits and labels
plt.ylim(-0.15, 1.3)
plt.xlim(xmin, xmax)
plt.xlabel('Position x')
plt.ylabel('Concentration C')
plt.title('Concentration Profiles at Different Times')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Add a horizontal line at y=0
plt.axvline(0, color='black', linewidth=0.5, linestyle='--')  # Add a vertical line at x=0
plt.axvline(xmax, color='black', linewidth=0.5, linestyle='--')  # Add a vertical line at x=L1
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot the PINN solution as a heatmap
plt.figure(figsize=(10, 6))
contour = plt.contourf(T, X, C_grid, 50, cmap='viridis')
plt.colorbar(contour, label='Concentration C(x,t)')
plt.xlabel('Time t')
plt.ylabel('Position x')
plt.title('PINN Solution C(x,t)')
plt.tight_layout()
plt.show()

# Plot the analytical solution as a heatmap
plt.figure(figsize=(10, 6))
contour = plt.contourf(T, X, C_act_grid, 50, cmap='viridis')
plt.colorbar(contour, label='Concentration C(x,t)')
plt.xlabel('Time t')
plt.ylabel('Position x')
plt.title('Analytical Solution C(x,t)')
plt.tight_layout()
plt.show()

# Plot the absolute error as a heatmap for the entire domain
plt.figure(figsize=(10, 6))
contour = plt.contourf(T, X, C_error_grid, 50, cmap='viridis')
plt.colorbar(contour, label='Absolute Error')
plt.xlabel('Time t')
plt.ylabel('Position x')
plt.title('Absolute Error of Analytical Solution and PINN Solution (Full Domain)')
plt.tight_layout()
plt.show()

# Added zoomed-in error maps for critical regions
# Create zoomed plots for early times and near boundary
# Focus on early time and space (t = 0 to 0.1, x = 0 to 0.1)
plt.figure(figsize=(12, 10))

# Plot 1: Early time solution comparison
plt.subplot(2, 2, 1)
early_t_idx = np.argmin(np.abs(t_vis - 0.05))  # t = 0.05
plt.plot(x_vis, C_grid[early_t_idx, :], label=f'Predicted t = 0.05')
plt.plot(x_vis, C_act_grid[early_t_idx, :], linestyle='--', label=f'Actual t = 0.05', alpha=0.7)
plt.ylim(-0.15, 1.3)
plt.xlim(xmin, xmax)
plt.xlabel('Position x')
plt.ylabel('Concentration C')
plt.title('Early Time Solution (t = 0.05)')
plt.legend()
plt.grid(True)

# Plot 2: Error distribution in early time and space [0, 0.1] × [0, 0.1]
plt.subplot(2, 2, 2)
# Create a mask for the early time region
early_region_mask = (T <= 0.1) & (X <= 0.1)
# Get only data in the early region
T_early = T * early_region_mask
T_early[T_early == 0] = np.nan  # Replace zeros with NaN to not plot them
X_early = X * early_region_mask
X_early[X_early == 0] = np.nan  # Replace zeros with NaN to not plot them
error_early = C_error_grid * early_region_mask
contour = plt.contourf(T_early, X_early, error_early, 50, cmap='viridis')
plt.colorbar(contour, label='Absolute Error')
plt.xlabel('Time t')
plt.ylabel('Position x')
plt.title('Error in Early Region (t ≤ 0.1, x ≤ 0.1)')

# Plot 3: Error along the left boundary x = 0
plt.subplot(2, 2, 3)
# Find the index corresponding to x = 0
x0_idx = 0  # Since x_vis starts at 0
# Plot error along x = 0 as a function of time
plt.plot(t_vis, C_error_grid[:, x0_idx])
plt.xlabel('Time t')
plt.ylabel('Absolute Error')
plt.title('Error Along Left Boundary (x = 0)')
plt.grid(True)

# Plot 4: Error along the initial condition t = 0
plt.subplot(2, 2, 4)
# Find the index corresponding to t = 0
t0_idx = 0  # Since t_vis starts at 0
# Plot error along t = 0 as a function of x
plt.plot(x_vis, C_error_grid[t0_idx, :])
plt.xlabel('Position x')
plt.ylabel('Absolute Error')
plt.title('Error Along Initial Condition (t = 0)')
plt.grid(True)

plt.tight_layout()
plt.show()

# Calculate and plot error metrics by region
# Define regions for analysis
regions = [
    {"name": "Full Domain", "x_range": (0, 1), "t_range": (0, 1)},
    {"name": "Early Region", "x_range": (0, 0.1), "t_range": (0, 0.1)},
    {"name": "Left Boundary", "x_range": (0, 0.05), "t_range": (0, 1)},
    {"name": "Right Boundary", "x_range": (0.95, 1), "t_range": (0, 1)}
]

# Create masks for each region
region_masks = []
for region in regions:
    x_min, x_max = region["x_range"]
    t_min, t_max = region["t_range"]
    mask = (X >= x_min) & (X <= x_max) & (T >= t_min) & (T <= t_max)
    region_masks.append(mask)

# Calculate error metrics for each region
region_metrics = []
for i, region in enumerate(regions):
    mask = region_masks[i]
    region_pred = C_grid[mask]
    region_act = C_act_grid[mask]
    region_error = C_error_grid[mask]
    
    mse = np.mean(region_error**2)
    rmse = np.sqrt(mse)
    max_error = np.max(region_error)
    
    region_metrics.append({
        "name": region["name"],
        "mse": mse,
        "rmse": rmse,
        "max_error": max_error
    })

# Plot the regional error metrics
plt.figure(figsize=(12, 6))
names = [m["name"] for m in region_metrics]
mse_values = [m["mse"] for m in region_metrics]
rmse_values = [m["rmse"] for m in region_metrics]
max_error_values = [m["max_error"] for m in region_metrics]

x = np.arange(len(names))
width = 0.25

plt.bar(x - width, mse_values, width, label='MSE')
plt.bar(x, rmse_values, width, label='RMSE')
plt.bar(x + width, max_error_values, width, label='Max Error')

plt.xlabel('Region')
plt.ylabel('Error Metric Value')
plt.title('Error Metrics by Region')
plt.xticks(x, names, rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

# Calculate error metrics for the whole domain
mse = np.mean((C_pred - C_act)**2)
rmse = np.sqrt(mse)
max_error = np.max(np.abs(C_pred - C_act))
r2 = r2_score(C_act, C_pred)

print(f"Mean Squared Error (MSE): {mse:.6e}")
print(f"Root Mean Squared Error (RMSE): {rmse:.6e}")
print(f"Maximum Absolute Error: {max_error:.6e}")
print(f"R-squared: {r2:.6f}")

# Print summary of changes made
print("\nSummary of Changes Made:")
print("1. Implemented hard constraint boundary condition using the formula: (base_nn*x)+(1.0-x)")
print("2. Eliminated boundary condition losses from the loss function")
print("3. Maintained neural network architecture with 5 hidden layers and 40 neurons per layer")
print("4. Kept the same collocation point distribution with 2,000 points")
print("5. Maintained the time-dependent weighting for better accuracy in early times")
